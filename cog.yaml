# Configuration for Cog ⚙️
# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md

image: "r8.im/afiaka87/argparse-to-cog-api"
build:
  # set to true if your model requires a GPU
  gpu: true

  # a list of ubuntu apt packages to install
  # system_packages:
    # - "libgl1-mesa-glx"
    # - "libglib2.0-0"

  # python version in the form '3.8' or '3.8.12'
  python_version: "3.8"

  # a list of packages in the format <package-name>==<version>
  python_packages:
    - "torch==1.9.0"
    - "transformers==4.16.2"
  
  # commands run after the environment is setup
  # maybe try installing huggingface stuff w python manually here

  run: 
    # run a huggingface import via python, so it is guaranteed to be in the image rather than downloaded at runtime.
    - python3.8 -m pip install transformers && python3.8 -c 'from transformers import GPT2TokenizerFast; t = GPT2TokenizerFast.from_pretrained("gpt2"); print(t.vocab_size)'


# predict.py defines how predictions are run on your model
predict: "predict.py:Predictor"